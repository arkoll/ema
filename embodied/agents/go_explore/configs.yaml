defaults:

    # Trainer
    log_wandb: True
    logdir: /dev/null
    run: train_gceval
    seed: 0
    task: dummy_discrete
    env: {amount: 4, parallel: process, daemon: False, repeat: 1, size: [64, 64], camera: -1, gray: False, length: 0, discretize: 0, lives: False, sticky: True, episodic: True, restart: True, again: False, termination: False, weaker: 1.0, seed: 0}
    replay: fixed
    replay_size: 2e6
    replay_chunk: 50
    replay_fixed: {prio_starts: 0.0, prio_ends: 1.0, sync: 0}
    replay_consec: {sync: 0}
    replay_prio: {prio_starts: 0.0, prio_ends: 1.0, sync: 0, fraction: 0.1, softmax: False, temp: 1.0, constant: 0.0, exponent: 0.5}
    tf: {jit: True, platform: gpu, precision: 16, debug_nans: False, logical_gpus: 0, dist_dataset: False, dist_policy: False, tensorfloat: True, placement: False, growth: True}
    eval_dir: ''
    filter: '.*'
    tbtt: 0
    train:
        steps: 1e7
        expl_until: 0
        log_every: 1e4
        eval_every: 3e4
        eval_eps: 1
        eval_samples: 1
        train_every: 5
        train_steps: 1
        train_fill: 1e4
        eval_fill: 1e4
        pretrain: 1
        log_zeros: False
        # log_keys_video: [image]
        log_keys_video: '^$'
        log_keys_sum: '^$'
        log_keys_mean: '^$'
        log_keys_max: '^$'
        log_timings: True
        sync_every: 180
    eval:
        repeat: 1
        threshold: 0.7
        goals: None

    # Agent
    task_behavior: PEG
    expl_behavior: None
    batch_size: 16
    transform_rewards: off
    expl_noise: 0.0
    eval_noise: 0.0
    eval_state_mean: False
    priority: reward_loss
    priority_correct: 0.0
    data_loader: tfdata

    # World Model
    grad_heads: [decoder]
    pred_reward: False
    pred_cont: False
    rssm: {units: 1024, deter: 1024, stoch: 32, classes: 32, act: elu, norm: layer, initial: learned2, unroll: True}
    encoder: {mlp_keys: '.*', cnn_keys: '.*', act: elu, norm: layer, mlp_layers: 4, mlp_units: 512, cnn: simple, cnn_depth: 64, cnn_kernels: [4, 4, 4, 4]}
    decoder: {mlp_keys: '.*', cnn_keys: '.*', act: elu, norm: layer, mlp_layers: 4, mlp_units: 512, cnn: simple, cnn_depth: 64, cnn_kernels: [5, 5, 6, 6], image_dist: mse, inputs: [deter, stoch]}
    reward_head: {layers: 4, units: 512, act: elu, norm: layer, dist: symlog, outscale: 0.1, inputs: [deter, stoch]}
    cont_head: {layers: 4, units: 512, act: elu, norm: layer, dist: binary, outscale: 0.1, inputs: [deter, stoch]}
    loss_scales: {kl: 1.0, image: 1.0, reward: 1.0, cont: 1.0}
    model_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2, wd_pattern: 'kernel'}
    wmkl: {impl: mult, scale: 0.1, target: 3.5, min: 1e-5, max: 1.0, vel: 0.1}
    wmkl_balance: 0.8

    # Actor Critic
    actor: {layers: 4, units: 400, act: elu, norm: layer, minstd: 0.03, maxstd: 1.0, outscale: 0.1, unimix: 0.01, inputs: [deter, stoch]}
    critic: {layers: 4, units: 400, act: elu, norm: layer, dist: symlog, outscale: 0.1, inputs: [deter, stoch]}
    actor_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2, wd_pattern: 'kernel'}
    critic_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2, wd_pattern: 'kernel'}
    actor_dist_disc: onehot
    actor_dist_cont: normal
    episodic: True
    discount: 0.99
    imag_discount: 0.99
    imag_horizon: 16
    imag_unroll: True
    critic_return: gve
    actor_return: gve
    return_lambda: 0.95
    actor_grad_disc: reinforce
    actor_grad_cont: backprop
    slow_target: True
    slow_target_update: 100
    slow_target_fraction: 1.0
    simple_ent: True
    actent: {impl: mult, scale: 1e-4, target: 0.1, min: 1e-5, max: 1e2, vel: 0.1}
    actent_norm: True
    actent_perdim: True
    advnorm: {impl: mean_std, decay: 0.99, max: 1e8}
    retnorm: {impl: std, decay: 0.999, max: 1e2}
    scorenorm: {impl: off, decay: 0.99, max: 1e8}
    adv_slow_critic: True
    pengs_qlambda: False
    critic_type: vfunction
    rewnorm_discount: False

    # PEG
    explorer: 'disag'
    worker: 'ac'
    train_goal: 'batch'
    worker_inputs: [deter, stoch, goal]
    worker_rews: {extr: 0.0, expl: 0.0, goal: 1.0}
    goal_reward: 'euclid'
    gc_duration: 25
    goal_strategy: 'Random'
    planner: {
        batch: 500,
        cem_elite_ratio: 0.2,
        cost_use_p2e_value: True,
        evaluate_only: False,
        final_step_cost: True,
        goal_min: [0.0],
        goal_max: [0.0],
        horizon: 50,
        init_candidates: [123456789.0],
        init_env_goal_percent: 0.0,
        mega_prior: False,
        mppi_gamma: 10.0,
        optimization_steps: 5,
        planner_type: shooting_mppi,
        repeat_samples: 0,
        std_scale: 1.0,
        sample_replay: False,
        sample_env_goal_percent: 0.0,
    }

    # Exploration
    expl_rewards: {extr: 0.0, disag: 1.0, vae: 0.0, ctrl: 0.0, pbe: 0.0}
    expl_discount: 0.99
    expl_retnorm: {impl: std, decay: 0.999, max: 1e8}
    expl_scorenorm: {impl: off, decay: 0.999, max: 1e8}
    disag_head: {layers: 4, units: 512, act: elu, norm: layer, dist: mse, inputs: [deter, stoch, action]}
    expl_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2}
    disag_target: [stoch]
    disag_models: 8
    ctrl_embed: {layers: 3, units: 512, act: elu, norm: layer, dist: mse, inputs: [deter, stoch]}
    ctrl_head: {layers: 1, units: 128, act: elu, norm: layer, dist: mse, inputs: [current, next]}
    ctrl_size: 32
    ctrl_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2}
    expl_enc: {layers: 4, units: 512, act: elu, norm: layer, dist: onehot, outscale: 0.1, inputs: [deter], shape: [8, 8]}
    expl_dec: {layers: 4, units: 512, act: elu, norm: layer, dist: mse, outscale: 0.1}
    expl_kl: {impl: mult, scale: 0.1, target: 10.0, min: 1e-2, max: 1.0, vel: 0.1}
    expl_vae_elbo: False

pointmaze:
    task: pointmaze_simplemaze
    grad_heads: [decoder]
    #rssm: {units: 20, deter: 20, stoch: 2, classes: 2, act: elu, norm: layer, initial: learned2, unroll: True}
    rssm: {units: 200, deter: 200, stoch: 32, classes: 32, act: elu, norm: layer, initial: learned2, unroll: True}
    encoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: 3, mlp_units: 400}
    decoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: 3, mlp_units: 400}
    train.steps: 1e6
    env.length: 50
    batch_size: 45

dmlab:

    task: dmlab_rooms_collect_good_objects_train
    encoder: {mlp_keys: 'reward', cnn_keys: 'image'}
    decoder: {mlp_keys: '$^', cnn_keys: 'image'}
    env.repeat: 4
    train.steps: 5e7

atari:

    task: atari_pong
    encoder: {mlp_keys: '$^', cnn_keys: 'image'}
    decoder: {mlp_keys: '$^', cnn_keys: 'image'}
    env: {gray: True, repeat: 4}
    train.steps: 5e7

crafter:

    task: crafter_reward
    encoder: {mlp_keys: '$^', cnn_keys: 'image'}
    decoder: {mlp_keys: '$^', cnn_keys: 'image'}
    train.log_keys_max: '^log_achievement_.*'
    train.log_keys_sum: '^log_reward$'

dmc_vision:

    task: dmc_walker_walk
    encoder: {mlp_keys: '$^', cnn_keys: 'image'}
    decoder: {mlp_keys: '$^', cnn_keys: 'image'}
    env.repeat: 1

dmc_proprio:

    task: dmc_walker_walk
    encoder: {mlp_keys: '.*', cnn_keys: '$^'}
    decoder: {mlp_keys: '.*', cnn_keys: '$^'}
    env.repeat: 1

pinpad:

    task: pinpad_five
    encoder: {mlp_keys: '$^', cnn_keys: 'image'}
    decoder: {mlp_keys: '$^', cnn_keys: 'image'}

loconav:

    task: loconav_ant_maze_m
    encoder: {mlp_keys: '.*(joints|sensors|actuator|effectors|appendages|bodies|height|zaxis).*', cnn_keys: 'image'}
    decoder: {mlp_keys: '.*(joints|sensors|actuator|effectors|appendages|bodies|height|zaxis).*', cnn_keys: 'image'}
    train.log_keys_max: '^log_.*'

debug:

    log_wandb: False
    env.length: 100
    env.restart: False
    env.amount: 2
    env.parallel: none
    train:
        eval_every: 300
        log_every: 300
        train_fill: 100
        eval_fill: 100
        train_steps: 1
        train_every: 30
        log_timings: False
    batch_size: 8
    replay_size: 500
    replay_chunk: 12
    encoder.cnn_depth: 16
    decoder.cnn_depth: 16
    rssm: {units: 64, stoch: 8, classes: 8}
    .*\.layers: 2
    .*\.units: 64
    .*\.wd: 0.0
    tf: {platform: gpu, jit: False}
